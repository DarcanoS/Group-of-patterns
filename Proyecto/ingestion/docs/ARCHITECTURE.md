# Architecture Overview - Ingestion Service

DocumentaciÃ³n visual de la arquitectura del servicio de ingestion.

---

## ðŸ—ï¸ Arquitectura General

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     INGESTION SERVICE                            â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   CLI      â”‚â”€â”€â”€â–¶â”‚  Ingestion   â”‚â”€â”€â”€â–¶â”‚  Database   â”‚         â”‚
â”‚  â”‚  (main.py) â”‚    â”‚   Service    â”‚    â”‚  (Session)  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                            â”‚                                     â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                    â”‚                â”‚                            â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚            â”‚   Adapters    â”‚  â”‚   Domain    â”‚                   â”‚
â”‚            â”‚  (Providers)  â”‚  â”‚   (DTOs)    â”‚                   â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚
         â”‚ reads                        â”‚ normalizes
         â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Sources   â”‚          â”‚  PostgreSQL + PostGISâ”‚
â”‚  - CSV files    â”‚          â”‚  - station           â”‚
â”‚  - AQICN API    â”‚          â”‚  - pollutant         â”‚
â”‚  - IoT sensors  â”‚          â”‚  - air_quality_readingâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”„ Flujo de Datos Completo

### Ingestion HistÃ³rica (CSV)

```
1. Inicio
   â”‚
   â”œâ”€â–¶ main.py --mode historical
   â”‚
   â–¼
2. Carga ConfiguraciÃ³n
   â”‚
   â”œâ”€â–¶ Lee station_mapping.yaml
   â”œâ”€â–¶ Carga .env (DATABASE_URL, etc.)
   â”‚
   â–¼
3. Crea Adapters
   â”‚
   â”œâ”€â–¶ Para cada estaciÃ³n en mapping:
   â”‚   â””â”€â–¶ HistoricalCsvAdapter(csv_path, metadata)
   â”‚
   â–¼
4. Fetch Readings (por cada adapter)
   â”‚
   â”œâ”€â–¶ Lee CSV con pandas
   â”œâ”€â–¶ Para cada fila:
   â”‚   â”œâ”€â–¶ Normaliza timestamp â†’ UTC
   â”‚   â”œâ”€â–¶ Para cada pollutant:
   â”‚   â”‚   â”œâ”€â–¶ Estandariza nombre (pm25 â†’ PM2.5)
   â”‚   â”‚   â”œâ”€â–¶ Valida rango (â‰¥0, â‰¤ max_threshold)
   â”‚   â”‚   â”œâ”€â–¶ Estima AQI (si es PM2.5)
   â”‚   â”‚   â””â”€â–¶ Crea NormalizedReading
   â”‚   â””â”€â–¶ Retorna List[NormalizedReading]
   â”‚
   â–¼
5. Persistencia (IngestionService)
   â”‚
   â”œâ”€â–¶ Para cada reading:
   â”‚   â”œâ”€â–¶ get_or_create_station()
   â”‚   â”‚   â”œâ”€â–¶ Busca en cache
   â”‚   â”‚   â”œâ”€â–¶ Si no existe, busca en DB
   â”‚   â”‚   â””â”€â–¶ Si no existe, crea nueva Station
   â”‚   â”‚
   â”‚   â”œâ”€â–¶ get_pollutant_id()
   â”‚   â”‚   â”œâ”€â–¶ Busca en cache
   â”‚   â”‚   â””â”€â–¶ Si no existe, busca en DB
   â”‚   â”‚
   â”‚   â”œâ”€â–¶ Verifica duplicado
   â”‚   â”‚   â””â”€â–¶ Query: (station_id, pollutant_id, datetime)
   â”‚   â”‚
   â”‚   â””â”€â–¶ Si no es duplicado:
   â”‚       â””â”€â–¶ INSERT INTO air_quality_reading
   â”‚
   â–¼
6. Commit & Resultados
   â”‚
   â”œâ”€â–¶ db.commit()
   â”œâ”€â–¶ Log estadÃ­sticas:
   â”‚   â”œâ”€â–¶ Lecturas fetched: X
   â”‚   â”œâ”€â–¶ Lecturas insertadas: Y
   â”‚   â””â”€â–¶ Lecturas omitidas: Z
   â”‚
   â””â”€â–¶ Exit 0
```

---

## ðŸŽ¨ Adapter Pattern - Secuencia de EjecuciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Clientâ”‚                â”‚IngestionSvc â”‚              â”‚ Adapter  â”‚
â”‚(main)â”‚                â”‚             â”‚              â”‚ (CSV)    â”‚
â””â”€â”€â”¬â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
   â”‚                           â”‚                          â”‚
   â”‚ run_historical_ingestion()â”‚                          â”‚
   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                          â”‚
   â”‚                           â”‚                          â”‚
   â”‚                           â”‚ create_historical_adapters()
   â”‚                           â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
   â”‚                           â”‚          â”‚               â”‚
   â”‚                           â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
   â”‚                           â”‚                          â”‚
   â”‚                           â”‚ fetch_readings()         â”‚
   â”‚                           â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
   â”‚                           â”‚                          â”‚
   â”‚                           â”‚                          â”‚â”€â”
   â”‚                           â”‚                          â”‚ â”‚ read CSV
   â”‚                           â”‚                          â”‚ â”‚ parse rows
   â”‚                           â”‚                          â”‚ â”‚ normalize
   â”‚                           â”‚                          â”‚ â”‚ validate
   â”‚                           â”‚                          â”‚â—€â”˜
   â”‚                           â”‚                          â”‚
   â”‚                           â”‚  List[NormalizedReading] â”‚
   â”‚                           â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
   â”‚                           â”‚                          â”‚
   â”‚                           â”‚â”€â”                        â”‚
   â”‚                           â”‚ â”‚ persist_readings()     â”‚
   â”‚                           â”‚ â”‚ - map to DB entities   â”‚
   â”‚                           â”‚ â”‚ - insert to PostgreSQL â”‚
   â”‚                           â”‚â—€â”˜                        â”‚
   â”‚                           â”‚                          â”‚
   â”‚    stats (success)        â”‚                          â”‚
   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                          â”‚
   â”‚                           â”‚                          â”‚
```

---

## ðŸ“Š TransformaciÃ³n de Datos

### De CSV a Base de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CSV File: carvajal,-bogota, colombia-air-quality.csv           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ date,       pm25,  pm10,  o3,  no2,  so2,  co                  â”‚
â”‚ 2019/10/2,  116,   47,    9,   14,   1,    11                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ HistoricalCsvAdapter
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NormalizedReading (x6, uno por cada pollutant)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ {                                                               â”‚
â”‚   external_station_id: "CARV-01",                              â”‚
â”‚   station_name: "Carvajal",                                    â”‚
â”‚   latitude: 4.614728,                                          â”‚
â”‚   longitude: -74.139465,                                       â”‚
â”‚   city: "BogotÃ¡",                                              â”‚
â”‚   country: "Colombia",                                         â”‚
â”‚   pollutant_code: "PM2.5",      â† Estandarizado                â”‚
â”‚   unit: "Âµg/mÂ³",                â† Estandarizado                â”‚
â”‚   value: 116.0,                                                â”‚
â”‚   aqi: 185,                     â† Calculado                    â”‚
â”‚   timestamp_utc: 2019-10-02T00:00:00+00:00  â† UTC             â”‚
â”‚ }                                                               â”‚
â”‚ ... (PM10, O3, NO2, SO2, CO)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ IngestionService
                           â”‚ - get_or_create_station()
                           â”‚ - get_pollutant_id()
                           â”‚ - check duplicates
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL: air_quality_reading                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id | station_id | pollutant_id | datetime            | value...â”‚
â”‚ 1  | 3          | 1            | 2019-10-02 00:00:00 | 116.0...â”‚
â”‚ 2  | 3          | 2            | 2019-10-02 00:00:00 | 47.0 ...â”‚
â”‚ 3  | 3          | 3            | 2019-10-02 00:00:00 | 9.0  ...â”‚
â”‚ ...                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ—„ï¸ Modelo de Datos

### Relaciones entre Entidades

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ map_region  â”‚
â”‚             â”‚
â”‚ id (PK)     â”‚
â”‚ name        â”‚
â”‚ geom        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 1:N
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ station         â”‚
â”‚                 â”‚
â”‚ id (PK)         â”‚
â”‚ name            â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ latitude        â”‚                    â”‚
â”‚ longitude       â”‚                    â”‚
â”‚ city            â”‚                    â”‚
â”‚ country         â”‚                    â”‚
â”‚ region_id (FK)  â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
       â”‚                               â”‚
       â”‚ 1:N                           â”‚
       â”‚                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚ air_quality_reading    â”‚             â”‚
â”‚                        â”‚             â”‚
â”‚ id (PK)                â”‚             â”‚
â”‚ station_id (FK)  â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚ pollutant_id (FK) â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚ datetime               â”‚ â”‚           â”‚
â”‚ value                  â”‚ â”‚           â”‚
â”‚ aqi                    â”‚ â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚           â”‚
                           â”‚           â”‚
                      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”‚
                      â”‚pollutantâ”‚      â”‚
                      â”‚         â”‚      â”‚
                      â”‚ id (PK) â”‚      â”‚
                      â”‚ name    â”‚      â”‚
                      â”‚ unit    â”‚      â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚ NormalizedReading    â”‚               â”‚
â”‚ (DTO - in memory)    â”‚â”€â”€â”€maps toâ”€â”€â”€â”€â”˜
â”‚                      â”‚
â”‚ external_station_id  â”‚
â”‚ pollutant_code       â”‚
â”‚ value                â”‚
â”‚ timestamp_utc        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“‚ Estructura de Archivos Anotada

```
ingestion/
â”‚
â”œâ”€â”€ ðŸ“„ .env.example              # Template de configuraciÃ³n
â”œâ”€â”€ ðŸ“„ requirements.txt          # Dependencias Python
â”œâ”€â”€ ðŸ“„ Dockerfile                # ContainerizaciÃ³n
â”œâ”€â”€ ðŸ“˜ README.md                 # DocumentaciÃ³n de uso
â”œâ”€â”€ ðŸ“˜ DESIGN_PATTERNS.md        # Este documento
â”‚
â”œâ”€â”€ ðŸ“‚ app/                      # CÃ³digo fuente
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸš€ main.py               # Entry point - CLI
â”‚   â”œâ”€â”€ âš™ï¸  config.py            # Settings (Pydantic)
â”‚   â”œâ”€â”€ ðŸ“ logging_config.py     # Logging setup
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“‚ db/                   # Capa de Base de Datos
â”‚   â”‚   â”œâ”€â”€ session.py           # âœ“ SQLAlchemy engine/session
â”‚   â”‚   â””â”€â”€ models.py            # âœ“ ORM models (Station, Pollutant, Reading)
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“‚ domain/               # LÃ³gica de Dominio
â”‚   â”‚   â”œâ”€â”€ dto.py               # âœ“ Pydantic DTOs (NormalizedReading)
â”‚   â”‚   â””â”€â”€ normalization.py    # âœ“ ConversiÃ³n, validaciÃ³n, AQI
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“‚ providers/            # ðŸŽ¨ ADAPTER PATTERN
â”‚   â”‚   â”œâ”€â”€ base_adapter.py            # âœ“ Interface abstracta
â”‚   â”‚   â”œâ”€â”€ historical_csv_adapter.py  # âœ“ CSV implementation
â”‚   â”‚   â””â”€â”€ aqicn_adapter.py           # ðŸ“‹ Futuro: API AQICN
â”‚   â”‚
â”‚   â””â”€â”€ ðŸ“‚ services/             # OrquestaciÃ³n
â”‚       â”œâ”€â”€ ingestion_service.py       # âœ“ Main orchestrator
â”‚       â””â”€â”€ aggregation_service.py     # ðŸ“‹ Futuro: Daily stats
â”‚
â””â”€â”€ ðŸ“‚ data/                     # ConfiguraciÃ³n
    â””â”€â”€ station_mapping.yaml     # Mapeo CSV â†’ Station metadata
```

**Leyenda**:
- âœ“ = Implementado
- ðŸ“‹ = Pendiente/Futuro
- ðŸŽ¨ = PatrÃ³n de diseÃ±o

---

## ðŸ”€ Casos de Uso

### Caso 1: Ingestion HistÃ³rica Inicial

**Escenario**: Primera vez ejecutando el servicio, base de datos vacÃ­a.

```
Entrada:
- 5 archivos CSV (~1,700 filas cada uno)
- station_mapping.yaml con 5 estaciones
- Base de datos con tabla pollutant poblada

Proceso:
1. Crea 5 HistoricalCsvAdapters
2. Por cada adapter:
   - Lee CSV (1,700 rows)
   - Genera ~10,200 NormalizedReadings (1,700 Ã— 6 pollutants)
3. Ingestion Service:
   - Crea 5 Stations nuevas
   - Inserta ~51,000 air_quality_readings (10,200 Ã— 5 stations)

Resultado:
âœ“ 5 stations creadas
âœ“ 51,000 readings insertadas
âœ“ 0 duplicados
â±ï¸ Tiempo estimado: 2-5 minutos
```

### Caso 2: Re-ingestion (con duplicados)

**Escenario**: Re-ejecutar el servicio con datos ya ingresados.

```
Entrada:
- Mismos 5 CSV
- Base de datos ya tiene las 51,000 readings

Proceso:
1. Crea adapters
2. Fetch readings (mismo proceso)
3. Ingestion Service:
   - Encuentra 5 stations existentes (usa cache)
   - Detecta 51,000 duplicados
   - Omite todas las inserciones

Resultado:
âœ“ 0 stations creadas
âœ“ 0 readings insertadas
âœ“ 51,000 duplicados detectados
â±ï¸ Tiempo: ~30 segundos (mÃ¡s rÃ¡pido, solo queries)
```

### Caso 3: Agregar Nueva EstaciÃ³n

**Escenario**: Se agrega un nuevo CSV con datos de una estaciÃ³n adicional.

```
Entrada:
- Nuevo CSV: kennedy-bogota.csv
- station_mapping.yaml actualizado con nueva estaciÃ³n

Proceso:
1. Crea 6 adapters (5 existentes + 1 nueva)
2. Adapters existentes:
   - Detectan duplicados, skip
3. Nuevo adapter:
   - Genera ~10,200 readings
   - Crea Station "Kennedy"
   - Inserta 10,200 readings nuevas

Resultado:
âœ“ 1 station creada (Kennedy)
âœ“ 10,200 readings insertadas
âœ“ 51,000 duplicados omitidos
â±ï¸ Tiempo: ~1 minuto
```

---

## ðŸ§© Extensibilidad

### Agregar Adapter para AQICN API

**Paso 1**: Implementar adapter

```python
# app/providers/aqicn_adapter.py

from app.providers.base_adapter import BaseExternalApiAdapter
import httpx

class AqicnAdapter(BaseExternalApiAdapter):
    """Adapter para AQICN JSON API"""
    
    def __init__(self, token: str, city: str):
        self.token = token
        self.city = city
        self.base_url = "https://api.waqi.info"
    
    def fetch_readings(self) -> List[NormalizedReading]:
        # 1. Call API
        response = httpx.get(
            f"{self.base_url}/feed/{self.city}/",
            params={"token": self.token}
        )
        data = response.json()
        
        # 2. Parse response
        readings = []
        for pollutant, value in data['data']['iaqi'].items():
            reading = NormalizedReading(
                external_station_id=f"aqicn-{data['data']['idx']}",
                station_name=data['data']['city']['name'],
                latitude=data['data']['city']['geo'][0],
                longitude=data['data']['city']['geo'][1],
                pollutant_code=standardize_pollutant_name(pollutant),
                unit=get_standard_unit(pollutant),
                value=value['v'],
                timestamp_utc=datetime.now(timezone.utc),
                aqi=data['data']['aqi']
            )
            readings.append(reading)
        
        return readings
```

**Paso 2**: Usar en servicio (sin cambios en cÃ³digo existente)

```python
# app/services/ingestion_service.py

def create_realtime_adapters(self) -> List[BaseExternalApiAdapter]:
    adapters = []
    
    for city in settings.ingestion_default_cities:
        adapter = AqicnAdapter(
            token=settings.aqicn_token,
            city=city
        )
        adapters.append(adapter)
    
    return adapters

def run_realtime_ingestion(self):
    adapters = self.create_realtime_adapters()  # Nueva fuente
    
    for adapter in adapters:
        readings = adapter.fetch_readings()  # âœ“ Misma interfaz
        self._persist_readings(readings)      # âœ“ Misma lÃ³gica
```

**Resultado**: Sin modificar `_persist_readings()` ni `main.py`, ahora soportamos API en tiempo real.

---

## ðŸŽ¯ Testing Strategy

### Unit Tests

```python
# tests/test_csv_adapter.py

def test_csv_adapter_implements_interface():
    """Verifica que implementa BaseExternalApiAdapter"""
    adapter = HistoricalCsvAdapter(...)
    assert isinstance(adapter, BaseExternalApiAdapter)
    assert hasattr(adapter, 'fetch_readings')

def test_csv_adapter_normalizes_timestamps():
    """Verifica conversiÃ³n a UTC"""
    adapter = HistoricalCsvAdapter(...)
    readings = adapter.fetch_readings()
    
    for reading in readings:
        assert reading.timestamp_utc.tzinfo == timezone.utc

def test_csv_adapter_validates_values():
    """Verifica que omite valores invÃ¡lidos"""
    # CSV con valor negativo
    adapter = HistoricalCsvAdapter(csv_with_invalid_data)
    readings = adapter.fetch_readings()
    
    # No debe haber valores negativos
    assert all(r.value >= 0 for r in readings)
```

### Integration Tests

```python
# tests/test_ingestion_service.py

def test_full_ingestion_pipeline(db_session):
    """Test end-to-end"""
    service = IngestionService(db_session)
    
    # Mock adapter
    mock_adapter = MockCsvAdapter(test_csv)
    
    # Run ingestion
    stats = service.run_historical_ingestion()
    
    # Verify database
    assert db_session.query(Station).count() > 0
    assert db_session.query(AirQualityReading).count() > 0
    assert stats['readings_inserted'] > 0
```

---

## ðŸ“ˆ Performance Considerations

### Optimizaciones Implementadas

1. **Caching**
   ```python
   self.station_cache: Dict[str, int] = {}
   self.pollutant_cache: Dict[str, int] = {}
   ```
   - Evita queries repetidas
   - ~80% reducciÃ³n en queries de lookup

2. **Bulk Processing**
   ```python
   for reading in readings:
       db.add(reading)
   db.flush()  # Una vez al final
   ```
   - Batch inserts
   - Reduce roundtrips a DB

3. **Duplicate Detection**
   ```python
   existing = db.query(...).filter(
       station_id == x,
       pollutant_id == y,
       datetime == z
   ).first()
   ```
   - Evita constraint violations
   - Permite re-runs seguros

### MÃ©tricas Esperadas

| OperaciÃ³n | Tiempo | Throughput |
|-----------|--------|------------|
| Fetch 1 CSV (1,700 rows) | ~2s | 850 rows/s |
| Normalize 10,200 readings | ~3s | 3,400 readings/s |
| Insert 10,200 readings (new) | ~15s | 680 inserts/s |
| Insert 10,200 readings (duplicates) | ~5s | 2,040 checks/s |
| **Total (5 stations, initial)** | **~2-5 min** | **~400 readings/s** |

---

## ðŸš€ Deployment

### Docker Compose Example

```yaml
version: '3.8'

services:
  ingestion-historical:
    build: ./ingestion
    environment:
      DATABASE_URL: postgresql://user:pass@postgres:5432/air_quality_db
      INGESTION_MODE: historical
    volumes:
      - ../data_air:/data_air:ro
    depends_on:
      - postgres
    command: ["python", "-m", "app.main", "--mode", "historical"]
  
  ingestion-realtime:
    build: ./ingestion
    environment:
      DATABASE_URL: postgresql://user:pass@postgres:5432/air_quality_db
      AQICN_TOKEN: ${AQICN_TOKEN}
      INGESTION_INTERVAL_MINUTES: 10
    depends_on:
      - postgres
    command: ["python", "-m", "app.main", "--mode", "realtime"]
```

---

## ðŸ“š ConclusiÃ³n

El servicio de ingestion implementa una arquitectura limpia, extensible y mantenible gracias a:

âœ… **Adapter Pattern**: Unifica mÃºltiples fuentes de datos  
âœ… **DTOs con Pydantic**: Type safety y validaciÃ³n  
âœ… **Arquitectura en capas**: SeparaciÃ³n de responsabilidades  
âœ… **SOLID principles**: CÃ³digo extensible sin modificaciones  
âœ… **Testing-friendly**: Componentes desacoplados

**PrÃ³ximos pasos**:
1. Implementar `AqicnAdapter` para tiempo real
2. Agregar tests automatizados
3. Implementar `AggregationService` para stats diarias
4. Agregar observabilidad (metrics, tracing)

---

**Ãšltima actualizaciÃ³n**: 26 de noviembre de 2025
